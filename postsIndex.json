[
  {
    "slug": "30-09-2025-ml-research-1",
    "title": "Machine Learning Research Notes 1 - Autoencoders and NMF",
    "date": "2025-09-30",
    "tags": [
      "university",
      "ML",
      "research",
      "autoencoders",
      "NMF",
      "SAE"
    ],
    "excerpt": "Compiled knowledge from papers and research on autoencoders and NMF.",
    "content": "This is the first post in a series where I plan to improve my writing abilities, as well as compile some of the knowledge I gather from reading papers, performing experiments and doing research on various machine learning topics.\n\n# Papers and Notes\n\n## The dynamics of representation learning in shallow, non-linear autoencoders - Maria Refinetti, Sebastian Goldt\n\nhttps://proceedings.mlr.press/v162/refinetti22a/refinetti22a.pdf\n\nThe main question of this paper I take out is the investigation whether nonlinear autoencoders can reach the PCA error, which the linear AE do by learning the principal components sequentially. With that said, as this is a paper that focuses on training of autoencoders, understanding (or at least attempting to) the decisions and the math behind it is a good learning opportunity to get the intuition for my own experiments later.\n\n### Definitions\n\n**Autoencoders (AE)** - class of neural networks trained to reconstruct their inputs.\n\n**Vanilla SGD** - selects only single point/batch of points to estimate the gradient at each step.\n\n**Bottleneck** - usually called the intermediate layer, as it often is significantly smaller than the input dimension. Forces the learning of compressed representation of inputs.\n\n**Shallow AE** - having single hiddden layer.\n\n**Tied AE** - where encoder and decoder weights are equal.\n\n**Teacher-Student setup** - smaller model learns to mimick larger pre-trained one.\n\n\n### Shallow Autoencoder - used in a paper\n\nGiven a $D$-dimensional input $\\bold{x} = (x_i)$ the output of the autoencoder is given by\n\n$$\n\\hat{x} = \\sum_k^Kv_i^kg(\\lambda^k)\\text{, } \\lambda^k\\equiv\\frac{\\bold{w}^k\\bold{x}}{\\sqrt{D}}\n$$\n\nwhere:\n\n- $\\bold{w}^k, \\bold{v}^k \\in \\mathbb{R}^D$ are the encoder and decoder weight of the $k$th hidden neurons respectively.\n- $g(\\cdot)$ is a function (either linear or non-linear).\n\nNote, this is not a general form, but a mathematical one specific to the paper's investigation.\n\n![image.png](assets/image.png)\n\nThe performance of a given autoencoder is measured by the **population reconstruction mean-squared error**:\n\n$$\n\\text{psme} \\equiv \\frac{1}{D}\\sum_i \\mathbb{E}(x_i - \\hat{x}_i)^2\n$$\n\n\n\n### Practical Implications\n\nThe paper investigates the sequential learning of principal components ir order of eigenvalue magnitude phenomenon. It shows the learning occurs in phases - the alignment phase where weights align to principal component directions, and the rescaling phase, where weight norms adjust to achieve PCA error.\n\nSome critical requirements mentioned are that sigmoidal AE with tied weights fail to achieve PCA error, and ReLU autoencoders require trainable biases to perform well. It is a readers (also mine) task to investigate why.\n\nFurthermore, the research notes the result hold remarkably well on real datasets like (CIFAR-10), as the derived models usesd synthetic Gaussian data.\n\n\n# Disentangling Dense Embeddings with Sparse Autoencoders - Charles O'Neill, Christine Ye, Kartheik Iyer, John F. Wu\n\nhttps://arxiv.org/abs/2408.00657\n\nThis is my starting point with Sparse Autoencoders, both delving deeper mathematically, as well as learning the practical application of disentangling features.\n\n### Practical Implications\n\nThe paper analyzes the effect of the nonlinear activation in the middle layer of an autoencoder. The math is too wild, but getting some of their intuition about what happens when we train autoencoders might be useful\n\nhttps://arxiv.org/abs/2408.00657\nThis paper is probably a good starting point for getting familiar with the sparse AE.\n\nhttps://www.cs.columbia.edu/~blei/fogm/2020F/readings/LeeSeung1999.pdf\nThe paper where they show the parts-based representation of NMF (easy to make a small experiment here, just load the olivetti face dataset, load some library that computes NMF, compute the NMF on the faces dataset, visualize the vectors that correspond to the nonnegative principal components - you get the parts based representations)."
  },
  {
    "slug": "2025-09-27-automatic-code-quality-checks",
    "title": "Automatic Code Quality checks according to SEP guidelines",
    "date": "2025-09-27",
    "tags": [
      "university",
      "github",
      "CI"
    ],
    "excerpt": "The set of steps needed to utilize Understand from Scitools and Simian checks within Github Actions.",
    "content": "# Purpose\n\nIn order to avoid either manual checks requiring each student to run the tools on their machines continuously throughout the project, or refactoring to adhere to the standards before submission, following this tutorial this process can be automated to have the checks continuously run on each pull request.\n\nThe guidelines we were given for our Software Engineering Project (SEP) are as follows:\n![img_1.png](2025-09-27-automatic-code-quality-checks/img_1.png)\n\nAnd most of these can be checked and reported on automatically.\nAn example in our frontend:\n![img.png](2025-09-27-automatic-code-quality-checks/img.png)\n\nIn this tutorial, I will guide through the steps needed to set up automatic code quality checks, on the example of our setup for frontend, but I specify each place where this template can be adjusted to your project.\n\nFor example in our repository, we have seperated the code into three folders:\n- `frontend/` - Angular frontend (JavaScript, TypeScript)\n- `backend/` - Flask backend (.NET)\n- 'model/' - ML models (Python)\nAnd thus created checks for each of these folders separately.\n\n# Prerequisites\n\n- A GitHub repository for your project. \n- Admin access to that repository to add secrets and modify Github Actions settings.\n(In case you chose to use GitLab or Bitbucket, the steps will be similar, but the tutorial is tailored to GH.)\n- Understand from Scitools license\n  (Single student rather than entire developing team needs to acquire it, it's reused on every run)\n\n# Steps\nTo quickly summarize the steps needed to set up the checks:\n1. Acquire a license for Understand from Scitools.\n2. Add the license to your repository secrets.\n3. Specify commands, paths, and languages to run Understand and Simian checks.\n4. Create a GitHub Actions workflow file.\n5. Few repo tweaks.\n\n## Step 1 - Acquire a license for Understand from Scitools\n\nYou can acquire a free license for Understand from Scitools as a student. Follow the instructions on their website: https://scitools.com/student\n\n## Step 2 - Add the license to your repository secrets\n\nOnce you have the license string, navigate to your GitHub repository, go to \"Settings\" > \"Secrets and variables\" > \"Actions\", and under \"Repository secrets\" add a new secret named `UNDERSTAND_LICENSE` with the license string as its value.\n\n## Step 3 - Add python script, specify commands, paths, and languages to run Understand and Simian checks\n\nBefore creating the workflow file, we need to prepare a few things in the repository.\n\n1. Create a folder `.github/quality_tools/` in your repository to store the necessary files.\n2. Add to that folder a Python script `parse_metrics.py` to parse the metrics CSV file generated by Understand into a markdown format suitable for GitHub comments. You can download the ready-made script here: [download parse_metrics.py](downloads/parse_metrics.py).\n\n> Place it in `.github/quality_tools/parse_metrics.py` in your repository.\n\nNext, we have to specify the commands for Understand to run the analysis. We create a template file named `frontend_commands.txt` in the same `.github/quality_tools/` folder.\n\n```aiignore\ncreate -languages Web frontend_metrics.und #TODO: Adjust the name and language\n-db frontend_metrics.und #TODO: Adjust the name if you have changed it above\nadd frontend/src\nsettings -MetricShowAggregatedFileMetrics on\nsettings -MetricShowAggregatedClassMetrics on\nsettings -MetricShowCouplingAndCohesionMetrics on\nsettings -MetricShowInheritanceMetrics on\nsettings -MetricShowStatementCountMetrics on\nsettings -MetricCyclomatic all\nsettings -MetricShowDefaultSummaryMetrics on\nanalyze\nmetrics\nreport\n```\nHere to adjust to your project, you might want to change:\n1. Language (e.g. `-languages C++` for C++ projects), we use 'Web' for our frontend project. List of supported languages can be found [here](https://support.scitools.com/support/solutions/articles/70000582794-supported-languages)\n2. Not needed, but you can also adjust the name of the database, but it should match in both places, and will have to be adjusted in templats later.\n\n## Step 4 - Create a GitHub Actions workflow file\n\nEither manually in a folder `.github/workflows/` or using the GitHub interface, create a new workflow file named `code_quality.yml`.\nThis will serve as the configuration for the CI pipeline, which we use to run checks but can be extended to do more in the future, this tutorial serves as a basis for creation just for code quality checks.\n\n\nHere I provide a sample workflow file for our frontend that you can use as a starting point. You can customize it according to your project's needs.\nLook at the comments (specifically those starting with `TODO`) for places where you might want to/should adjust the configuration.\n\n[download frontend_code_quality_job.yml](downloads/frontend_code_quality_job.yml)\n\n> Make sure it is in `.github/workflows` folder.\n\n## Step 5 - Last repo tweaks\n\n1. Go to your repository settings, under \"Actions\" > \"General\", and ensure that \"Read and write permissions\" is selected for the workflow permissions. This is necessary for the workflow to post comments on pull requests.\n\n# Done!\n\nOnce you have completed these steps, every time a pull request is created or updated in your repository, the GitHub Actions workflow will automatically run the code quality checks using Understand and Simian. The results will be posted as a comment on the pull request, allowing you to easily see any issues that need to be addressed before merging.\n\nAdditionally, you can inspect the results by downloading the artifact uploaded by the job when it succeeds:\n![img_2.png](2025-09-27-automatic-code-quality-checks/img_2.png)\n\nUnzip and open the `index.html` file in your browser to see the full report:\n![img_3.png](2025-09-27-automatic-code-quality-checks/img_3.png)\n\n> Note: This template is to be extended with similarity analyzer, if you want the update you can reach out to me at - m.derylo@student.tue.nl."
  }
]