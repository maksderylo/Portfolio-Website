[
  {
    "slug": "20-10-2025-marathon-2",
    "title": "I ran a second marathon",
    "date": "2025-10-20",
    "tags": [
      "running",
      "marathon"
    ],
    "excerpt": "Unfortunately, underwhelming time - 3:46:51",
    "content": "Unfortunately, underwhelming time - 3:46:51. Goal for next time is sub 3:30.\n\n![EHAA3029-original.jpeg](assets/EHAA3029-original.jpeg)\n![EHNK15920-original.jpeg](assets/EHNK15920-original.jpeg)\n![EHXH3407-original.jpeg](assets/EHXH3407-original.jpeg)\n![EHXJ5029-original.jpeg](assets/EHXJ5029-original.jpeg)"
  },
  {
    "slug": "14-10-2025-ml-research-3",
    "title": "Machine Learning Research Notes 3 - SAE and NMF comparison",
    "date": "2025-10-14",
    "tags": [
      "university",
      "ML",
      "research",
      "SAE",
      "NMF"
    ],
    "excerpt": "Under what conditions do SAE and NMF converge to similar solutions?",
    "content": "This week I will focus on wrapping up the knowledge gathered about NMF and SAE, while also trying to focus on the parametrization of both in order to get similar parts based representation results.\n\n# Quick Wrap-up\n\n## NMF\n\nNMF decomposes a non-negative data matrix $D$ into two non-negative matrices $W$ (basis) and $H$ (coefficients) such that:\n\n$$\nD \\approx WH\n$$\n\n, the standard formulation of the optimization problem is:\n\n$$\n\\min_{W,H} ||D - WH||^2_F\n$$\n\nsubject to $W \\geq 0, H \\geq 0$.\n\n## SAE\n\n1. Forward pass:\n\n- Encoder: $H_{pre} = DU^T$, where $D$ is the data matrix, $U$ the encoder weights\n- Activation: $H = f(H_{pre})$ (e.g. ReLU, jumpReLU, TopK)\n- Reconstruction: $\\hat{D} = YX^T$, where $Y$ is the decoder weights\n\n2. Backward Pass: The model updates weights via gradient descent on the reconstruction loss:\n\n$$\n\\mathcal{L} = ||D - \\hat{D}||^2 + \\lambda \\cdot \\text{sparsity\\_loss}(H)\n$$\n\nwhere $\\lambda$ is a hyperparameter controlling the influence of the sparsity loss (e.g. KL-Divergence).\n\nThus, for SAE without the sparsity loss, but with TopK hard constraint, the SAE in terms of optimization can be written down as\n\n$$\n\\min_{U,X}||D-\\hat{D}||^2_2\n$$\n\n$$\n\\text{s.t. }\\hat{D} =HX^T,\n$$\n\n$$\nH=JumpReLU(DU^T),\n$$\n\n$$\n||H_{i\\cdot}||_0 = k, \\forall i\n$$\n\nwhere $D$ is the original data, $\\hat{D}$ is the reconstruction, $U$ and $X$ the encoder and decoder weights respectively.\n\n# Comparison of NMF and SAE\n\nhttps://arxiv.org/pdf/1601.02733\n\nhttps://arxiv.org/html/2411.02124v1\n\n### New Definitions and Terms\n\n- **Variational Autoencoder (VAE)**: A type of autoencoder that learns a probabilistic mapping from input data to a latent space, allowing for the generation of new data samples by sampling from the learned distribution. Also used for disentanglement - https://openreview.net/forum?id=Sy2fzU9gl\n- **NNSAE** - Nonnegative Sparse Autoencoder: An autoencoder that incorporates non-negativity constraints on the weights and activations, promoting parts-based representations similar to Non-negative Matrix Factorization (NMF).\n- **NCAE** - Nonnegative Constrained Autoencoder - https://arxiv.org/pdf/1601.02733"
  },
  {
    "slug": "08-10-2025-ml-research-2",
    "title": "Machine Learning Research Notes 2 - Understanding SAE",
    "date": "2025-10-08",
    "tags": [
      "university",
      "ML",
      "research",
      "autoencoders",
      "SAE",
      "PyTorch"
    ],
    "excerpt": "Deeveloping emperical understanding of SAE, what is going on, effects of parameters into learned features.",
    "content": "This week's task is to gain \"the empirical understanding of what is going on\". I will work on coding my own SAE in a way I can easily tweak and visualize the effects of different parameters. In other words, I will be learning PyTorch - a deep learning framework commonly perceived as more appropriate for research compared to Tensorflow.\n\nI think an important distinction of the common use case of SAE to the one here should be made. Usually, SAE are used to disentangle dense representations (e.g. word embeddings) into more interpretable ones, while here the goal is to learn disentangled and interpretable features directly from the data (MNIST handwritten numbers).\n\n# Initial attempt and tests\n\nTo find out how to implement SAE I looked over some tutorials and simply searched for implementations and started with 2 versions:\n\n- Geeks for Geeks - Tensorflow - https://www.geeksforgeeks.org/deep-learning/sparse-autoencoders-in-deep-learning/\n- ChatGPT implementation - Pytorch (you can find in my gh)\n\nWhat I immediately noticed was the loss calculation.\n\n```python\ndef sparse_loss(y_true, y_pred):\n    mse_loss = tf.reduce_mean(keras.losses.MeanSquaredError()(y_true, y_pred))\n    hidden_layer_output = encoder(y_true)\n    mean_activation = tf.reduce_mean(hidden_layer_output, axis=0)\n\n    kl_divergence = tf.reduce_sum(sparsity_level * tf.math.log(sparsity_level / (mean_activation + 1e-10)) +\n                                  (1 - sparsity_level) * tf.math.log((1 - sparsity_level) / (1 - mean_activation + 1e-10)))\n\n    return mse_loss + lambda_sparse * kl_divergence\n```\n\nFor the sparsity they use the kl_divergence Kullback–Leibler divergence $D_{KL} = \\sum \\left( \\rho \\log \\frac{\\rho}{\\hat{\\rho} + \\epsilon} + (1 - \\rho) \\log \\frac{1 - \\rho}{1 - \\hat{\\rho} + \\epsilon} \\right)$ [wikipedia](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence). When put in the loss function it instead of forcing sparsity (small number of non-zero activations) it rather penalizes the model for not having the average activation close to the desired sparsity level. This is a bit counter-intuitive, but seems to be common approach.\n\nVisualization of the encoder/decoder weights after training:\n![img.png](06-10-2025-ml-research-2/img20.png)\n![img.png](06-10-2025-ml-research-2/img.png)\nWhere the decoder weights do show some interpretable patterns, the encoder weights on the other hand are quite noisy.\n\nBut importantly, since the implementation forces the mean to be low, there are many active hidden units and the actual sparsity is pretty low:\n![img_1.png](06-10-2025-ml-research-2/img_1.png)\n\n# My own implementation\n\nAs mentioned before, I decided to implement my own SAE in PyTorch. Deciding on the definition of the training objective I went to the paper from my last [post](https://www.maksderylo.com/#/blog/30-09-2025-ml-research-1) - **Disentangling Dense Embeddings with Sparse Autoencoders - Charles O'Neill, Christine Ye, Kartheik Iyer, John F. Wu**, where they use $k$-sparse constraint.\n\n## Conceptual Inconsistency in the Paper?\n\nQuoting the paper:\n![img_2.png](06-10-2025-ml-research-2/img_2.png)\n\nI believe there is a conceptual inconsistency here. Using the $k$-sparse constraint should not be in the loss function, as it is a hard constraint. Instead of penalizing activation of non k-largest activations, the smaller ones are zeroed out, thus the lambda variable seems useless.\n\n> Note: After further investigation, the original paper this one cites indeeds clarifies this inconsistency like so:\n![img_13.png](06-10-2025-ml-research-2/img_13.png) where it correctly distinguishes between TopK and L1 sparse autoencoders. \n\n## Implementation \n\nMoving forward, for now I disregard the auxiliary loss and implement the forward pass like so:\n\n``` python \n    def forward(self, x):\n\n        # Encoder: linear projection\n        h_pre = self.encoder(x)\n\n        # Apply ReLU to ensure non-negative activations\n        h_pre = F.relu(h_pre)\n\n        # TopK activation: keep only k largest activations per sample\n        topk_vals, topk_idx = torch.topk(h_pre, self.k_param, dim=1)\n\n        # Create sparse activation tensor\n        h = torch.zeros_like(h_pre)\n        h.scatter_(1, topk_idx, topk_vals)\n\n        # Decoder: reconstruct input\n        x_hat = self.decoder(h)\n\n        return h, x_hat\n```\nhttps://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html\nhttps://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html\n\nand the loss calculation is a simple MSE.\n\n## Results\n\nI decided to train with different values of k 10 and for comparison 64 features, while keeping other parameters constant:\n\n```python\nlearning_rate=0.0005,\nnum_epochs=50,\nbatch_size=256,\nhidden_size=64,\n```\n\n### k=10\n![img_3.png](06-10-2025-ml-research-2/img_3.png)\n![img_4.png](06-10-2025-ml-research-2/img_4.png)\n\nWhat shows quickly is the large number of dead features (>50%), but these encoder decoder weights show clear distinguishable patterns. Then each reconstruction is made with at most 10 features, which is quite impressive.\n![img_5.png](06-10-2025-ml-research-2/img_5.png)\n![img_6.png](06-10-2025-ml-research-2/img_6.png)\n\nThe main takeaway is that the model is not utilizing the full potential of available features, but the ones utilized are humanly interpretable.\n\n### k=64\n\nTo compare, I trained the same model with k=64, which means no sparsity constraint at all. The results are quite different, for example the encoder weights look like so:\n![img_7.png](06-10-2025-ml-research-2/img_7.png)\nWhich again contains dead features (2 in this case), but the rest of the features are not humanly interpretable.\n![img_8.png](06-10-2025-ml-research-2/img_8.png)\n![img_9.png](06-10-2025-ml-research-2/img_9.png)\nOn the other hand, what one might have expected, when the model is less constrained, the reconstructions are better.\n\n## Smaller latent space?\n\nWith the large amount of dead features, I wondered if the model would perform similarly given that the latent space is smaller. Repeating the experiment with k=10, about 50% of features were dead, thus I trained the model with hidden size of 32 and k=10. The results are quite similar to the previous k=10 experiment, but now only 9 features are dead.\n![img_10.png](06-10-2025-ml-research-2/img_10.png)\n![img_11.png](06-10-2025-ml-research-2/img_11.png)\n![img_12.png](06-10-2025-ml-research-2/img_12.png)\n\nIt shows the reconstructions got worse although the amount of active features didn't drop drastically (23 active vs 30 active previously). Going even further down e.g. latent space = 15, the number of dead features seems to go to zero, but the reconstructions are visibly worse.\n\n# Going further\n\nI think the next steps would be:\n- Tackle dead features - learn the usage of auxiliary loss, initialization of encoder and decoder from the paper or other methods.\n- Try different datasets and modality - e.g. faces, text embeddings.\n  \n\nhttps://cdn.openai.com/papers/sparse-autoencoders.pdf\nhttps://openai.com/index/extracting-concepts-from-gpt-4/"
  },
  {
    "slug": "06-10-2025-book-recom-mans-search-for-meaning",
    "title": "Book Recommendation - Man's Search for Meaning - Viktor Frankl",
    "date": "2025-10-06",
    "tags": [
      "book",
      "recommendation"
    ],
    "excerpt": "!img.png",
    "content": "![img.png](06-10-2025-book-recom-mans-search-for-meaning/img.png)"
  },
  {
    "slug": "30-09-2025-ml-research-1",
    "title": "Machine Learning Research Notes 1 - Autoencoders and NMF",
    "date": "2025-09-30",
    "tags": [
      "university",
      "ML",
      "research",
      "autoencoders",
      "NMF",
      "SAE"
    ],
    "excerpt": "Compiled knowledge from papers and research on autoencoders and NMF.",
    "content": "This is the first post in a series where I plan to improve my writing abilities, as well as compile some of the knowledge I gather from reading papers, performing experiments and doing research on various machine learning topics.\n\n# Papers and Notes\n\n## The dynamics of representation learning in shallow, non-linear autoencoders - Maria Refinetti, Sebastian Goldt\n\nhttps://proceedings.mlr.press/v162/refinetti22a/refinetti22a.pdf\n\nThe main question of this paper I take out is the investigation whether nonlinear autoencoders can reach the PCA error, which the linear AE do by learning the principal components sequentially. With that said, as this is a paper that focuses on training of autoencoders, understanding (or at least attempting to) the decisions and the math behind it is a good learning opportunity to get the intuition for my own experiments later.\n\n### Definitions\n\n**Autoencoders (AE)** - class of neural networks trained to reconstruct their inputs.\n\n**Vanilla SGD** - selects only single point/batch of points to estimate the gradient at each step.\n\n**Bottleneck** - usually called the intermediate layer, as it often is significantly smaller than the input dimension. Forces the learning of compressed representation of inputs.\n\n**Shallow AE** - having single hidden layer.\n\n**Tied AE** - where encoder and decoder weights are equal.\n\n**Teacher-Student setup** - smaller model learns to mimic larger pre-trained one.\n\n### Shallow Autoencoder - used in a paper\n\nGiven a $D$-dimensional input $\\bold{x} = (x_i)$ the output of the autoencoder is given by\n\n$$\n\\hat{x} = \\sum_k^K v_i^k g(\\lambda^k)\\text{, } \\lambda^k\\equiv\\frac{\\bold{w}^k\\bold{x}}{\\sqrt{D}}\n$$\n\nwhere:\n\n- $\\bold{w}^k, \\bold{v}^k \\in \\mathbb{R}^D$ are the encoder and decoder weight of the $k$th hidden neurons respectively.\n- $g(\\cdot)$ is a function (either linear or non-linear).\n\nNote, this is not a general form, but a mathematical one specific to the paper's investigation.\n\n![image.png](assets/image.png)\n\nThe performance of a given autoencoder is measured by the **population reconstruction mean-squared error**:\n\n$$\n\\text{psme} \\equiv \\frac{1}{D}\\sum_i \\mathbb{E}(x_i - \\hat{x}_i)^2\n$$\n\n### Practical Implications\n\nThe paper investigates the sequential learning of principal components ir order of eigenvalue magnitude phenomenon. It shows the learning occurs in phases - the alignment phase where weights align to principal component directions, and the rescaling phase, where weight norms adjust to achieve PCA error.\n\nSome critical requirements mentioned are that sigmoidal AE with tied weights fail to achieve PCA error, and ReLU autoencoders require trainable biases to perform well. It is a readers (also mine) task to investigate why.\n\nFurthermore, the research notes the result hold remarkably well on real datasets like (CIFAR-10), as the derived models use synthetic Gaussian data.\n\n## Disentangling Dense Embeddings with Sparse Autoencoders - Charles O'Neill, Christine Ye, Kartheik Iyer, John F. Wu\n\nhttps://arxiv.org/abs/2408.00657\n\nThis is my starting point with Sparse Autoencoders, both delving deeper mathematically, and learning the practical application of disentangling features. Specifically, due to operations on relatively small latent spaces, activations of neurons are tangled leading to the \"everything activates everything\" problem. Using sparse AE, having larger latent space than the input and output, as well as adding sparsity to the objective, allow to retrieve the meaning behind encoded features.\n\n![image.png](assets/image5.png)\n\nThis paper aims to fill the gap by analyzing the application of SAE to dense text embeddings developing a model for interoperability, (or as above - autointerpretability - which I less focus on) of features and relationships between them. Importantly, it provides background on embedding representation and sparse autoencoders, thus .\n\n## Definitions\n\n**Sparse autoencoder (SAE)** - class of autoencoders with a sparse set of features (semantic concepts) in a higher-dimensional space, potentially disentangling superposed features.\n\n**One-hot encodings** - vectors with a single 1 for category and 0 for the rest - equidistant.\n\n**Dense vector embedding** - continuous, low-dimensional vectors.\n\n**Ablation** - systematic removal or modification of a component (such as a layer, feature, or module) in a model to evaluate its impact on overall performance.\n\n### Sparse Autoencoders\n\nLet $\\bold{x} \\in \\mathbb{R}^d$ be an input vector, and $\\bold{h} \\in \\mathbb{R}^n$ be the hidden representation, where typically $n \\gg d$ (much greater than). The encoder and decodder functions are defined as:\n\n$$\n\\begin{aligned}\n\\text{Encoder: }\\quad \\bold{h} &= f_\\theta(\\bold{x}) = \\sigma(W_e \\bold{x} + \\bold{b}_e) \\\\\n\\text{Decoder: }\\quad \\hat{\\bold{x}} &= g_\\phi(\\bold{h}) = W_d \\bold{h} + \\bold{b}_d\n\\end{aligned}\n$$\n\nwhere $W_e \\in \\mathbb{R}^{n \\times d}$ and $W_d \\in \\mathbb{R}^{d \\times n}$ are encoding and decoding weight matrices and their corresponding bias vectors $b_e \\in \\mathbb{R}^n$ and $b_d \\in \\mathbb{R}^d$, $\\sigma(\\cdot)$ is a non-linear activation function (like ReLU or sigmoid).\n\nThe training objective of SAE combines three components:\n\n- Reconstruction loss\n- Sparsity constraint\n- Sometimes (here) auxiliary loss\n\n$$\n\\mathcal{L}(\\theta,\\phi) = \\frac{1}{d} \\Vert \\bold{x-\\hat{x}} \\Vert_2^2 + \\lambda \\mathcal{L}_{sparse}(h) + \\alpha \\mathcal{L}_{aux}(x,\\hat{x})\n$$\n\nwhere $\\lambda > 0$ and $\\alpha > 0$ are hyperparameters controlling the trade-off between reconstruction fidelity, sparsity, and the auxiliary loss.\n\nAs the sparsity constraint, they use a $k$-sparse constraint - only $k$ largest activations in $\\bold{h}$ are retained.\n\nAs the auxiliary loss, they use a technique to remove non activating \"dead\" latents. Latents are flagged as dead during training if they have not activated for a predetermined number of tokens. Given the reconstruction error of the main model $\\bold{e = x -\\hat{x}}$, they define the auxiliary loss as:\n\n$$\n\\mathcal{L}_{aux}(\\bold{x,\\hat{x}}) = \\Vert \\bold{e - \\hat{e}} \\Vert_2^2\n$$\n\nwhere $\\bold{\\hat{e}} =W_d\\bold{z}$ is the reconstruction using the top $k_{aux} = 2k$ (twice the number of active latents) dead latents, and $\\bold{z}$ is the sparse representation using only these dead latents. This additional loss term helps to revive dead features and improve overall representational capacity of the model.\n\n## Learning the parts of object by non-negative matrix factorization - Daniel D. Lee, H. Sebastian Seung\n\nhttps://www.cs.columbia.edu/~blei/fogm/2020F/readings/LeeSeung1999.pdf\n\nIn this paper researchers introduce Non-negative Matrix Factorization (NMF) as a method to learn parts-based representation of data. Unlike traditional methods like PCA or vector quantization, which often produce holistic representations, NMF yields a decomposition where both the basis components and the coefficients are non-negative, leading to more interpretable parts-based representations.\nWith that, it is an inspiration for my own experiments with NMF, as well as a good starting point for understanding the method and its applications.\n\n### Definitions\n\n**Non-negative Matrix Factorization (NMF)** - a group of algorithms in multivariate analysis and linear algebra where a matrix $V$ is factorized into (usually) two matrices $W$ and $H$, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect.\n**Holistic representation** - a representation where each component captures global features of the data, often leading to entangled and less interpretable features.\n**VQ** - vector quantization.\n**PCA** - principal component analysis.\n\n### NMF, VQ and PCA formulations\n\nGiven a non-negative data matrix $V$ of dimensions $n \\times m$, NMF seeks to find two non-negative matrices $W$ (of dimensions $m \\times r$) and $H$ (of dimensions $r \\times n$) such that:\n\n$$\nV \\approx WH\n$$\n\n$$\nV_{i\\mu} \\approx \\sum_{a=1}^r W_{ia}H_{a\\mu}\n$$\n\nWhere in the case of image data, $V_{i\\mu}$ represents the intensity of pixel $i$ in image $\\mu$, $W_{ia}$ represents the contribution of pixel $i$ to feature $a$, and $H_{a\\mu}$ represents the activation of feature $a$ in image $\\mu$.\nThe $r$ columns of $W$ are called basis images. Each column of $H$ is called an encoding and is one-to-one correspondence with a face in $V$.\nThe dimensions of the matrices $W$ and $H$, that are $n\\times r$ and $r\\times m$, respectively, are chosen such that $r < \\min(m,n)$, leading to a compressed representation of the data.\n\nThe differences between NMF, VQ and PCA are due to different constrains imposed on $W$ and $H$:\n\n- **VQ** - constrains $H$ to be a binary matrix with exactly one non-zero entry per column(unary vectors), and $W$. This means each data point is represented by a single basis vector.\n- **PCA** - allows both positive and negative entries in $W$ and $H$, leading to holistic representations where each basis vector can contribute positively or negatively to the reconstruction of data points.\n- **NMF** - constrains both $W$ and $H$ to be non-negative, leading to parts-based representations where basis vectors represent additive components of the data.\n  ![img_4.png](assets/img_4.png)\n\n### Repeating the NMF experiment\n\nAfter reading the paper, I performed a few experiments trying to repeat the experiments with my own code:\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_olivetti_faces\nfrom sklearn.decomposition import NMF\n\n# 1. Load the Olivetti faces dataset\n\nfaces = fetch_olivetti_faces(shuffle=True, random_state=42)\nX = faces.data\nimage_shape = faces.images[0].shape\n\n# 2. Apply NMF\n\nn_components = 16\nnmf = NMF(n_components=n_components, init='random', random_state=42, max_iter=10000)\nW = nmf.fit_transform(X)\nH = nmf.components_\n\n# 3. Visualize the NMF components (parts-based representation)\n\nfig, axes = plt.subplots(2, n_components // 2, figsize=(12, 4),\nsubplot_kw={'xticks':[], 'yticks':[]})\nfor i, ax in enumerate(axes.flat):\nax.imshow(H[i].reshape(image_shape), cmap='gray')\nax.set_title(f'Component {i+1}')\nplt.tight_layout()\nplt.show()\n```\n\nThis code loads the Olivetti faces dataset, applies NMF to extract 16 components, and visualizes these components as images. I expected the resulting components should represent parts of faces, such as eyes, noses, and mouths, similar to the results presented in the paper.\n\n### Initial experiment\n\nThe result was:\n![img.png](assets/img.png)\n\nThis doesn't confirm the results from the paper, as the components are clear faces, not parts of faces. The initial possible reasons for this discrepancy could be:\n\n- Different initialization method for NMF.\n- Different number of components.\n- Smaller dataset used.\n  Or the combination of these factors.\n\n### Addressing the potential issues\n\nTo address these issues, I made the following adjustments:\n\n1. Changed the number of components to 36.\n2. Changed the initialization method to 'nndsvda', which is known to yield better results for NMF.\n\n```python\nn_components = 36\nnmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=10000)\nW = nmf.fit_transform(X)\nH = nmf.components_\n\nfig, axes = plt.subplots(6, 6, figsize=(10, 10),\n                         subplot_kw={'xticks':[], 'yticks':[]})\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(H[i].reshape(image_shape), cmap='gray')\n    ax.set_title(f'{i+1}', fontsize=8)\nplt.tight_layout()\nplt.show()\n```\n\nThe updated result was:\n![img.png](assets/img1.png)\nWhich is closer to the expected parts-based representation, showing distinct facial features in some of the components.\n\nThe last change was the size of the dataset. The Olivetti dataset contains only 400 images, which might be insufficient for NMF to learn meaningful parts, compared to 2,429 faces used in the paper. To address this, I switched to the LFW (Labeled Faces in the Wild) dataset, which contains over 7,000 images of faces.\n\nWith that, I prepared the experiment comparing the convergence of NMF as the number of training samples increased, to see whether the data size/feature to data size did play a role in extracting more specific features.\n\n```python\n# python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import NMF\n\ndef visualize_components_by_size(\n    X_train, image_shape, sample_sizes, n_components=64,\n    max_iter=1000, random_state=0\n):\n    assert np.prod(image_shape) == X_train.shape[1], \"image_shape does not match feature dimension.\"\n    assert n_components == 64, \"For an 8x8 grid, set n_components=64.\"\n\n    rng = np.random.RandomState(123)\n\n    for size in sorted(s for s in sample_sizes if s > 0):\n        size = min(size, len(X_train))\n        idx = rng.choice(len(X_train), size=size, replace=False)\n        X_sub = X_train[idx]\n\n        nmf = NMF(\n            n_components=n_components,\n            init='nndsvda',\n            random_state=random_state,\n            max_iter=max_iter,\n            tol=1e-4\n        )\n        nmf.fit(X_sub)\n        H = nmf.components_\n\n        fig, axes = plt.subplots(8, 8, figsize=(10, 10),\n                                 subplot_kw={'xticks': [], 'yticks': []})\n        for i, ax in enumerate(axes.flat):\n            ax.imshow(H[i].reshape(image_shape), cmap='gray')\n            ax.set_title(f'{i+1}', fontsize=8)\n\n        fig.suptitle(f'NMF components | samples={size} | k={n_components}', y=0.98, fontsize=10)\n        plt.tight_layout()\n        plt.show()\n\n# Ensure image_shape matches LFW\nimage_shape = lfw.images.shape[1:]\nassert np.prod(image_shape) == X_train.shape[1]\n```\n\nAnd last thing was to call the function with appropriate parameters:\n\n```python\nvisualize_components_by_size(\n    X_train, image_shape, sample_sizes=[400, len(X_train)],\n    n_components=64, max_iter=6000, random_state=0\n)\n```\n\nWhich resulted in:\n![img.png](assets/img2.png)\n![img.png](assets/img4.png)\n\nFinally, I tested the MSE and reconstruction error as the sample size grew, for the number of components 64, as well as increased the maximum number of iterations to 60,000 to ensure convergence.\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_lfw_people\nfrom sklearn.decomposition import NMF\nfrom sklearn.model_selection import train_test_split\n\n# 1) Load LFW and filter identities with at least 2 images (needed for stratification)\nlfw = fetch_lfw_people(min_faces_per_person=0, resize=0.5, color=False)\nX = lfw.data.astype(np.float32)\ny = lfw.target\n\ncounts = np.bincount(y)\nkeep_labels = np.where(counts >= 2)[0]\nmask = np.isin(y, keep_labels)\nX, y = X[mask], y[mask]\n\n# 2) Fixed held-out test set (stratified now safe)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# 3) Evaluate NMF as training samples increase (unstratified subsampling to avoid per-class count issues)\ndef eval_nmf(sample_sizes, n_components=64, max_iter=60000, random_state=0):\n    rng = np.random.RandomState(random_state)\n    results = []\n    for size in sorted(s for s in sample_sizes if s > 0):\n        size = min(size, len(X_train))\n        idx = rng.choice(len(X_train), size=size, replace=False)\n        X_sub = X_train[idx]\n\n        nmf = NMF(\n            n_components=n_components,\n            init='nndsvda',\n            random_state=random_state,\n            max_iter=max_iter,\n            tol=1e-4\n        )\n        W_train = nmf.fit_transform(X_sub)\n        H = nmf.components_\n\n        W_test = nmf.transform(X_test)\n        X_test_hat = W_test @ H\n        mse = float(np.mean((X_test - X_test_hat) ** 2))\n        results.append((size, mse))\n        print(f\"samples={size} | test MSE={mse:.6f} | train recon_err={nmf.reconstruction_err_:.6f}\")\n    return results\n\nsample_sizes = [200, 500, 1000, 2000, 4000, 8000, len(X_train)]\nresults = eval_nmf(sample_sizes, n_components=64, max_iter=60000, random_state=0)\n\n# 4) Plot learning curve\nsizes, mses = zip(*results)\nplt.figure(figsize=(5, 3))\nplt.plot(sizes, mses, marker='o')\nplt.xlabel('training samples')\nplt.ylabel('test reconstruction MSE')\nplt.title('NMF performance vs. samples')\nplt.grid(True, ls='--', alpha=0.4)\nplt.tight_layout()\nplt.show()\n```\n\nThe output was:\n\n\n| samples      | test MSE          | test recon_err             |\n| ------------ | ----------------- | -------------------------- |\n| 200  | 0.006207 | 40.446190  |\n| 500  | 0.004738 | 71.699516  |\n| 1000 | 0.004332 | 107.500862 |\n| 2000 | 0.004177 | 153.365433 |\n| 4000 | 0.004107 | 216.769272 |\n| 7331 | 0.004082 | 295.135712 |\n| 7331 | 0.004080 | 295.10900  |\n\n![img.png](assets/img3.png)\n\n### Conclusions\n\nNMF behavior is sensitive to:\n\n- Initialization (NNDSVDA accelerates convergence and yields more localized, additive parts).\n- Number of components (too few => holistic faces; more components => emergent parts like eyes, shadows, mouth regions).\n- Dataset size (larger sample sets reduce test MSE and allow finer specialization; gains plateau as curve flattens).\n\n\n  Even with more data and components, some bases remain quasi-holistic—indicating limits of plain Frobenius NMF without added sparsity or alignment constraints.\n  Empirically, increasing components + structured init + sufficient data shifts representations from global templates toward additive parts, consistent with the parts-based hypothesis motivating NMF and sparse feature learning."
  },
  {
    "slug": "2025-09-27-automatic-code-quality-checks",
    "title": "Automatic Code Quality checks according to SEP guidelines",
    "date": "2025-09-27",
    "tags": [
      "university",
      "github",
      "CI"
    ],
    "excerpt": "The set of steps needed to utilize Understand from Scitools and Simian checks within Github Actions.",
    "content": "# Purpose\n\nIn order to avoid either manual checks requiring each student to run the tools on their machines continuously throughout the project, or refactoring to adhere to the standards before submission, following this tutorial this process can be automated to have the checks continuously run on each pull request.\n\nThe guidelines we were given for our Software Engineering Project (SEP) are as follows:\n![img_1.png](2025-09-27-automatic-code-quality-checks/img_1.png)\n\nAnd most of these can be checked and reported on automatically.\nAn example in our frontend:\n![img.png](2025-09-27-automatic-code-quality-checks/img.png)\n\nIn this tutorial, I will guide through the steps needed to set up automatic code quality checks, on the example of our setup for frontend, but I specify each place where this template can be adjusted to your project.\n\nFor example in our repository, we have seperated the code into three folders:\n- `frontend/` - Angular frontend (JavaScript, TypeScript)\n- `backend/` - Flask backend (.NET)\n- 'model/' - ML models (Python)\nAnd thus created checks for each of these folders separately.\n\n# Prerequisites\n\n- A GitHub repository for your project. \n- Admin access to that repository to add secrets and modify Github Actions settings.\n(In case you chose to use GitLab or Bitbucket, the steps will be similar, but the tutorial is tailored to GH.)\n- Understand from Scitools license\n  (Single student rather than entire developing team needs to acquire it, it's reused on every run)\n\n# Steps\nTo quickly summarize the steps needed to set up the checks:\n1. Acquire a license for Understand from Scitools.\n2. Add the license to your repository secrets.\n3. Specify commands, paths, and languages to run Understand and Simian checks.\n4. Create a GitHub Actions workflow file.\n5. Few repo tweaks.\n\n## Step 1 - Acquire a license for Understand from Scitools\n\nYou can acquire a free license for Understand from Scitools as a student. Follow the instructions on their website: https://scitools.com/student\n\n## Step 2 - Add the license to your repository secrets\n\nOnce you have the license string, navigate to your GitHub repository, go to \"Settings\" > \"Secrets and variables\" > \"Actions\", and under \"Repository secrets\" add a new secret named `UNDERSTAND_LICENSE` with the license string as its value.\n\n## Step 3 - Add python script, specify commands, paths, and languages to run Understand and Simian checks\n\nBefore creating the workflow file, we need to prepare a few things in the repository.\n\n1. Create a folder `.github/quality_tools/` in your repository to store the necessary files.\n2. Add to that folder a Python script `parse_metrics.py` to parse the metrics CSV file generated by Understand into a markdown format suitable for GitHub comments. You can download the ready-made script here: [download parse_metrics.py](downloads/parse_metrics.py).\n\n> Place it in `.github/quality_tools/parse_metrics.py` in your repository.\n\nNext, we have to specify the commands for Understand to run the analysis. We create a template file named `frontend_commands.txt` in the same `.github/quality_tools/` folder.\n\n```aiignore\ncreate -languages Web frontend_metrics.und #TODO: Adjust the name and language\n-db frontend_metrics.und #TODO: Adjust the name if you have changed it above\nadd frontend/src\nsettings -MetricShowAggregatedFileMetrics on\nsettings -MetricShowAggregatedClassMetrics on\nsettings -MetricShowCouplingAndCohesionMetrics on\nsettings -MetricShowInheritanceMetrics on\nsettings -MetricShowStatementCountMetrics on\nsettings -MetricCyclomatic all\nsettings -MetricShowDefaultSummaryMetrics on\nanalyze\nmetrics\nreport\n```\nHere to adjust to your project, you might want to change:\n1. Language (e.g. `-languages C++` for C++ projects), we use 'Web' for our frontend project. List of supported languages can be found [here](https://support.scitools.com/support/solutions/articles/70000582794-supported-languages)\n2. Not needed, but you can also adjust the name of the database, but it should match in both places, and will have to be adjusted in templats later.\n\n## Step 4 - Create a GitHub Actions workflow file\n\nEither manually in a folder `.github/workflows/` or using the GitHub interface, create a new workflow file named `code_quality.yml`.\nThis will serve as the configuration for the CI pipeline, which we use to run checks but can be extended to do more in the future, this tutorial serves as a basis for creation just for code quality checks.\n\n\nHere I provide a sample workflow file for our frontend that you can use as a starting point. You can customize it according to your project's needs.\nLook at the comments (specifically those starting with `TODO`) for places where you might want to/should adjust the configuration.\n\n[download frontend_code_quality_job.yml](downloads/frontend_code_quality_job.yml)\n\n> Make sure it is in `.github/workflows` folder.\n\n## Step 5 - Last repo tweaks\n\n1. Go to your repository settings, under \"Actions\" > \"General\", and ensure that \"Read and write permissions\" is selected for the workflow permissions. This is necessary for the workflow to post comments on pull requests.\n\n# Done!\n\nOnce you have completed these steps, every time a pull request is created or updated in your repository, the GitHub Actions workflow will automatically run the code quality checks using Understand and Simian. The results will be posted as a comment on the pull request, allowing you to easily see any issues that need to be addressed before merging.\n\nAdditionally, you can inspect the results by downloading the artifact uploaded by the job when it succeeds:\n![img_2.png](2025-09-27-automatic-code-quality-checks/img_2.png)\n\nUnzip and open the `index.html` file in your browser to see the full report:\n![img_3.png](2025-09-27-automatic-code-quality-checks/img_3.png)\n\n> Note: This template is to be extended with similarity analyzer, if you want the update you can reach out to me at - m.derylo@student.tue.nl."
  }
]