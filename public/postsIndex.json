[
  {
    "slug": "30-09-2025-ml-research-1",
    "title": "Machine Learning Research Notes 1 - Autoencoders and NMF",
    "date": "2025-09-30",
    "tags": [
      "university",
      "ML",
      "research",
      "autoencoders",
      "NMF",
      "SAE"
    ],
    "excerpt": "Compiled knowledge from papers and research on autoencoders and NMF.",
    "content": "This is the first post in a series where I plan to improve my writing abilities, as well as compile some of the knowledge I gather from reading papers, performing experiments and doing research on various machine learning topics.\n\n# Papers and Notes\n\n## The dynamics of representation learning in shallow, non-linear autoencoders - Maria Refinetti, Sebastian Goldt\n\nhttps://proceedings.mlr.press/v162/refinetti22a/refinetti22a.pdf\n\nThe main question of this paper I take out is the investigation whether nonlinear autoencoders can reach the PCA error, which the linear AE do by learning the principal components sequentially. With that said, as this is a paper that focuses on training of autoencoders, understanding (or at least attempting to) the decisions and the math behind it is a good learning opportunity to get the intuition for my own experiments later.\n\n### Definitions\n\n**Autoencoders (AE)** - class of neural networks trained to reconstruct their inputs.\n\n**Vanilla SGD** - selects only single point/batch of points to estimate the gradient at each step.\n\n**Bottleneck** - usually called the intermediate layer, as it often is significantly smaller than the input dimension. Forces the learning of compressed representation of inputs.\n\n**Shallow AE** - having single hidden layer.\n\n**Tied AE** - where encoder and decoder weights are equal.\n\n**Teacher-Student setup** - smaller model learns to mimic larger pre-trained one.\n\n### Shallow Autoencoder - used in a paper\n\nGiven a $D$-dimensional input $\\bold{x} = (x_i)$ the output of the autoencoder is given by\n\n$$\n\\hat{x} = \\sum_k^K v_i^k g(\\lambda^k)\\text{, } \\lambda^k\\equiv\\frac{\\bold{w}^k\\bold{x}}{\\sqrt{D}}\n$$\n\nwhere:\n\n- $\\bold{w}^k, \\bold{v}^k \\in \\mathbb{R}^D$ are the encoder and decoder weight of the $k$th hidden neurons respectively.\n- $g(\\cdot)$ is a function (either linear or non-linear).\n\nNote, this is not a general form, but a mathematical one specific to the paper's investigation.\n\n![image.png](assets/image.png)\n\nThe performance of a given autoencoder is measured by the **population reconstruction mean-squared error**:\n\n$$\n\\text{psme} \\equiv \\frac{1}{D}\\sum_i \\mathbb{E}(x_i - \\hat{x}_i)^2\n$$\n\n### Practical Implications\n\nThe paper investigates the sequential learning of principal components ir order of eigenvalue magnitude phenomenon. It shows the learning occurs in phases - the alignment phase where weights align to principal component directions, and the rescaling phase, where weight norms adjust to achieve PCA error.\n\nSome critical requirements mentioned are that sigmoidal AE with tied weights fail to achieve PCA error, and ReLU autoencoders require trainable biases to perform well. It is a readers (also mine) task to investigate why.\n\nFurthermore, the research notes the result hold remarkably well on real datasets like (CIFAR-10), as the derived models use synthetic Gaussian data.\n\n## Disentangling Dense Embeddings with Sparse Autoencoders - Charles O'Neill, Christine Ye, Kartheik Iyer, John F. Wu\n\nhttps://arxiv.org/abs/2408.00657\n\nThis is my starting point with Sparse Autoencoders, both delving deeper mathematically, and learning the practical application of disentangling features. Specifically, due to operations on relatively small latent spaces, activations of neurons are tangled leading to the \"everything activates everything\" problem. Using sparse AE, having larger latent space than the input and output, as well as adding sparsity to the objective, allow to retrieve the meaning behind encoded features.\n\n![image.png](assets/image5.png)\n\nThis paper aims to fill the gap by analyzing the application of SAE to dense text embeddings developing a model for interoperability, (or as above - autointerpretability - which I less focus on) of features and relationships between them. Importantly, it provides background on embedding representation and sparse autoencoders, thus .\n\n## Definitions\n\n**Sparse autoencoder (SAE)** - class of autoencoders with a sparse set of features (semantic concepts) in a higher-dimensional space, potentially disentangling superposed features.\n\n**One-hot encodings** - vectors with a single 1 for category and 0 for the rest - equidistant.\n\n**Dense vector embedding** - continuous, low-dimensional vectors.\n\n**Ablation** - systematic removal or modification of a component (such as a layer, feature, or module) in a model to evaluate its impact on overall performance.\n\n### Sparse Autoencoders\n\nLet $\\bold{x} \\in \\mathbb{R}^d$ be an input vector, and $\\bold{h} \\in \\mathbb{R}^n$ be the hidden representation, where typically $n \\gg d$ (much greater than). The encoder and decodder functions are defined as:\n\n$$\n\\begin{aligned}\n\\text{Encoder: }\\quad \\bold{h} &= f_\\theta(\\bold{x}) = \\sigma(W_e \\bold{x} + \\bold{b}_e) \\\\\n\\text{Decoder: }\\quad \\hat{\\bold{x}} &= g_\\phi(\\bold{h}) = W_d \\bold{h} + \\bold{b}_d\n\\end{aligned}\n$$\n\nwhere $W_e \\in \\mathbb{R}^{n \\times d}$ and $W_d \\in \\mathbb{R}^{d \\times n}$ are encoding and decoding weight matrices and their corresponding bias vectors $b_e \\in \\mathbb{R}^n$ and $b_d \\in \\mathbb{R}^d$, $\\sigma(\\cdot)$ is a non-linear activation function (like ReLU or sigmoid).\n\nThe training objective of SAE combines three components:\n\n- Reconstruction loss\n- Sparsity constraint\n- Sometimes (here) auxiliary loss\n\n$$\n\\mathcal{L}(\\theta,\\phi) = \\frac{1}{d} \\Vert \\bold{x-\\hat{x}} \\Vert_2^2 + \\lambda \\mathcal{L}_{sparse}(h) + \\alpha \\mathcal{L}_{aux}(x,\\hat{x})\n$$\n\nwhere $\\lambda > 0$ and $\\alpha > 0$ are hyperparameters controlling the trade-off between reconstruction fidelity, sparsity, and the auxiliary loss.\n\nAs the sparsity constraint, they use a $k$-sparse constraint - only $k$ largest activations in $\\bold{h}$ are retained.\n\nAs the auxiliary loss, they use a technique to remove non activating \"dead\" latents. Latents are flagged as dead during training if they have not activated for a predetermined number of tokens. Given the reconstruction error of the main model $\\bold{e = x -\\hat{x}}$, they define the auxiliary loss as:\n\n$$\n\\mathcal{L}_{aux}(\\bold{x,\\hat{x}}) = \\Vert \\bold{e - \\hat{e}} \\Vert_2^2\n$$\n\nwhere $\\bold{\\hat{e}} =W_d\\bold{z}$ is the reconstruction using the top $k_{aux} = 2k$ (twice the number of active latents) dead latents, and $\\bold{z}$ is the sparse representation using only these dead latents. This additional loss term helps to revive dead features and improve overall representational capacity of the model.\n\n## Learning the parts of object by non-negative matrix factorization - Daniel D. Lee, H. Sebastian Seung\n\nhttps://www.cs.columbia.edu/~blei/fogm/2020F/readings/LeeSeung1999.pdf\n\nIn this paper researchers introduce Non-negative Matrix Factorization (NMF) as a method to learn parts-based representation of data. Unlike traditional methods like PCA or vector quantization, which often produce holistic representations, NMF yields a decomposition where both the basis components and the coefficients are non-negative, leading to more interpretable parts-based representations.\nWith that, it is an inspiration for my own experiments with NMF, as well as a good starting point for understanding the method and its applications.\n\n### Definitions\n\n**Non-negative Matrix Factorization (NMF)** - a group of algorithms in multivariate analysis and linear algebra where a matrix $V$ is factorized into (usually) two matrices $W$ and $H$, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect.\n**Holistic representation** - a representation where each component captures global features of the data, often leading to entangled and less interpretable features.\n**VQ** - vector quantization.\n**PCA** - principal component analysis.\n\n### NMF, VQ and PCA formulations\n\nGiven a non-negative data matrix $V$ of dimensions $n \\times m$, NMF seeks to find two non-negative matrices $W$ (of dimensions $m \\times r$) and $H$ (of dimensions $r \\times n$) such that:\n\n$$\nV \\approx WH\n$$\n\n$$\nV_{i\\mu} \\approx \\sum_{a=1}^r W_{ia}H_{a\\mu}\n$$\n\nWhere in the case of image data, $V_{i\\mu}$ represents the intensity of pixel $i$ in image $\\mu$, $W_{ia}$ represents the contribution of pixel $i$ to feature $a$, and $H_{a\\mu}$ represents the activation of feature $a$ in image $\\mu$.\nThe $r$ columns of $W$ are called basis images. Each column of $H$ is called an encoding and is one-to-one correspondence with a face in $V$.\nThe dimensions of the matrices $W$ and $H$, that are $n\\times r$ and $r\\times m$, respectively, are chosen such that $r < \\min(m,n)$, leading to a compressed representation of the data.\n\nThe differences between NMF, VQ and PCA are due to different constrains imposed on $W$ and $H$:\n\n- **VQ** - constrains $H$ to be a binary matrix with exactly one non-zero entry per column(unary vectors), and $W$. This means each data point is represented by a single basis vector.\n- **PCA** - allows both positive and negative entries in $W$ and $H$, leading to holistic representations where each basis vector can contribute positively or negatively to the reconstruction of data points.\n- **NMF** - constrains both $W$ and $H$ to be non-negative, leading to parts-based representations where basis vectors represent additive components of the data.\n  ![img_4.png](assets/img_4.png)\n\n### Repeating the NMF experiment\n\nAfter reading the paper, I performed a few experiments trying to repeat the experiments with my own code:\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_olivetti_faces\nfrom sklearn.decomposition import NMF\n\n# 1. Load the Olivetti faces dataset\n\nfaces = fetch_olivetti_faces(shuffle=True, random_state=42)\nX = faces.data\nimage_shape = faces.images[0].shape\n\n# 2. Apply NMF\n\nn_components = 16\nnmf = NMF(n_components=n_components, init='random', random_state=42, max_iter=10000)\nW = nmf.fit_transform(X)\nH = nmf.components_\n\n# 3. Visualize the NMF components (parts-based representation)\n\nfig, axes = plt.subplots(2, n_components // 2, figsize=(12, 4),\nsubplot_kw={'xticks':[], 'yticks':[]})\nfor i, ax in enumerate(axes.flat):\nax.imshow(H[i].reshape(image_shape), cmap='gray')\nax.set_title(f'Component {i+1}')\nplt.tight_layout()\nplt.show()\n```\n\nThis code loads the Olivetti faces dataset, applies NMF to extract 16 components, and visualizes these components as images. I expected the resulting components should represent parts of faces, such as eyes, noses, and mouths, similar to the results presented in the paper.\n\n### Initial experiment\n\nThe result was:\n![img.png](assets/img.png)\n\nThis doesn't confirm the results from the paper, as the components are clear faces, not parts of faces. The initial possible reasons for this discrepancy could be:\n\n- Different initialization method for NMF.\n- Different number of components.\n- Smaller dataset used.\n  Or the combination of these factors.\n\n### Addressing the potential issues\n\nTo address these issues, I made the following adjustments:\n\n1. Changed the number of components to 36.\n2. Changed the initialization method to 'nndsvda', which is known to yield better results for NMF.\n\n```python\nn_components = 36\nnmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=10000)\nW = nmf.fit_transform(X)\nH = nmf.components_\n\nfig, axes = plt.subplots(6, 6, figsize=(10, 10),\n                         subplot_kw={'xticks':[], 'yticks':[]})\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(H[i].reshape(image_shape), cmap='gray')\n    ax.set_title(f'{i+1}', fontsize=8)\nplt.tight_layout()\nplt.show()\n```\n\nThe updated result was:\n![img.png](assets/img1.png)\nWhich is closer to the expected parts-based representation, showing distinct facial features in some of the components.\n\nThe last change was the size of the dataset. The Olivetti dataset contains only 400 images, which might be insufficient for NMF to learn meaningful parts, compared to 2,429 faces used in the paper. To address this, I switched to the LFW (Labeled Faces in the Wild) dataset, which contains over 7,000 images of faces.\n\nWith that, I prepared the experiment comparing the convergence of NMF as the number of training samples increased, to see whether the data size/feature to data size did play a role in extracting more specific features.\n\n```python\n# python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import NMF\n\ndef visualize_components_by_size(\n    X_train, image_shape, sample_sizes, n_components=64,\n    max_iter=1000, random_state=0\n):\n    assert np.prod(image_shape) == X_train.shape[1], \"image_shape does not match feature dimension.\"\n    assert n_components == 64, \"For an 8x8 grid, set n_components=64.\"\n\n    rng = np.random.RandomState(123)\n\n    for size in sorted(s for s in sample_sizes if s > 0):\n        size = min(size, len(X_train))\n        idx = rng.choice(len(X_train), size=size, replace=False)\n        X_sub = X_train[idx]\n\n        nmf = NMF(\n            n_components=n_components,\n            init='nndsvda',\n            random_state=random_state,\n            max_iter=max_iter,\n            tol=1e-4\n        )\n        nmf.fit(X_sub)\n        H = nmf.components_\n\n        fig, axes = plt.subplots(8, 8, figsize=(10, 10),\n                                 subplot_kw={'xticks': [], 'yticks': []})\n        for i, ax in enumerate(axes.flat):\n            ax.imshow(H[i].reshape(image_shape), cmap='gray')\n            ax.set_title(f'{i+1}', fontsize=8)\n\n        fig.suptitle(f'NMF components | samples={size} | k={n_components}', y=0.98, fontsize=10)\n        plt.tight_layout()\n        plt.show()\n\n# Ensure image_shape matches LFW\nimage_shape = lfw.images.shape[1:]\nassert np.prod(image_shape) == X_train.shape[1]\n```\n\nAnd last thing was to call the function with appropriate parameters:\n\n```python\nvisualize_components_by_size(\n    X_train, image_shape, sample_sizes=[400, len(X_train)],\n    n_components=64, max_iter=6000, random_state=0\n)\n```\n\nWhich resulted in:\n![img.png](assets/img2.png)\n![img.png](assets/img4.png)\n\nFinally, I tested the MSE and reconstruction error as the sample size grew, for the number of components 64, as well as increased the maximum number of iterations to 60,000 to ensure convergence.\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_lfw_people\nfrom sklearn.decomposition import NMF\nfrom sklearn.model_selection import train_test_split\n\n# 1) Load LFW and filter identities with at least 2 images (needed for stratification)\nlfw = fetch_lfw_people(min_faces_per_person=0, resize=0.5, color=False)\nX = lfw.data.astype(np.float32)\ny = lfw.target\n\ncounts = np.bincount(y)\nkeep_labels = np.where(counts >= 2)[0]\nmask = np.isin(y, keep_labels)\nX, y = X[mask], y[mask]\n\n# 2) Fixed held-out test set (stratified now safe)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# 3) Evaluate NMF as training samples increase (unstratified subsampling to avoid per-class count issues)\ndef eval_nmf(sample_sizes, n_components=64, max_iter=60000, random_state=0):\n    rng = np.random.RandomState(random_state)\n    results = []\n    for size in sorted(s for s in sample_sizes if s > 0):\n        size = min(size, len(X_train))\n        idx = rng.choice(len(X_train), size=size, replace=False)\n        X_sub = X_train[idx]\n\n        nmf = NMF(\n            n_components=n_components,\n            init='nndsvda',\n            random_state=random_state,\n            max_iter=max_iter,\n            tol=1e-4\n        )\n        W_train = nmf.fit_transform(X_sub)\n        H = nmf.components_\n\n        W_test = nmf.transform(X_test)\n        X_test_hat = W_test @ H\n        mse = float(np.mean((X_test - X_test_hat) ** 2))\n        results.append((size, mse))\n        print(f\"samples={size} | test MSE={mse:.6f} | train recon_err={nmf.reconstruction_err_:.6f}\")\n    return results\n\nsample_sizes = [200, 500, 1000, 2000, 4000, 8000, len(X_train)]\nresults = eval_nmf(sample_sizes, n_components=64, max_iter=60000, random_state=0)\n\n# 4) Plot learning curve\nsizes, mses = zip(*results)\nplt.figure(figsize=(5, 3))\nplt.plot(sizes, mses, marker='o')\nplt.xlabel('training samples')\nplt.ylabel('test reconstruction MSE')\nplt.title('NMF performance vs. samples')\nplt.grid(True, ls='--', alpha=0.4)\nplt.tight_layout()\nplt.show()\n```\n\nThe output was:\n\n\n| samples      | test MSE          | test recon_err             |\n| ------------ | ----------------- | -------------------------- |\n| 200  | 0.006207 | 40.446190  |\n| 500  | 0.004738 | 71.699516  |\n| 1000 | 0.004332 | 107.500862 |\n| 2000 | 0.004177 | 153.365433 |\n| 4000 | 0.004107 | 216.769272 |\n| 7331 | 0.004082 | 295.135712 |\n| 7331 | 0.004080 | 295.10900  |\n\n![img.png](assets/img3.png)\n\n### Conclusions\n\nNMF behavior is sensitive to:\n\n- Initialization (NNDSVDA accelerates convergence and yields more localized, additive parts).\n- Number of components (too few => holistic faces; more components => emergent parts like eyes, shadows, mouth regions).\n- Dataset size (larger sample sets reduce test MSE and allow finer specialization; gains plateau as curve flattens).\n\n\n  Even with more data and components, some bases remain quasi-holistic—indicating limits of plain Frobenius NMF without added sparsity or alignment constraints.\n  Empirically, increasing components + structured init + sufficient data shifts representations from global templates toward additive parts, consistent with the parts-based hypothesis motivating NMF and sparse feature learning."
  },
  {
    "slug": "2025-09-27-automatic-code-quality-checks",
    "title": "Automatic Code Quality checks according to SEP guidelines",
    "date": "2025-09-27",
    "tags": [
      "university",
      "github",
      "CI"
    ],
    "excerpt": "The set of steps needed to utilize Understand from Scitools and Simian checks within Github Actions.",
    "content": "# Purpose\n\nIn order to avoid either manual checks requiring each student to run the tools on their machines continuously throughout the project, or refactoring to adhere to the standards before submission, following this tutorial this process can be automated to have the checks continuously run on each pull request.\n\nThe guidelines we were given for our Software Engineering Project (SEP) are as follows:\n![img_1.png](2025-09-27-automatic-code-quality-checks/img_1.png)\n\nAnd most of these can be checked and reported on automatically.\nAn example in our frontend:\n![img.png](2025-09-27-automatic-code-quality-checks/img.png)\n\nIn this tutorial, I will guide through the steps needed to set up automatic code quality checks, on the example of our setup for frontend, but I specify each place where this template can be adjusted to your project.\n\nFor example in our repository, we have seperated the code into three folders:\n- `frontend/` - Angular frontend (JavaScript, TypeScript)\n- `backend/` - Flask backend (.NET)\n- 'model/' - ML models (Python)\nAnd thus created checks for each of these folders separately.\n\n# Prerequisites\n\n- A GitHub repository for your project. \n- Admin access to that repository to add secrets and modify Github Actions settings.\n(In case you chose to use GitLab or Bitbucket, the steps will be similar, but the tutorial is tailored to GH.)\n- Understand from Scitools license\n  (Single student rather than entire developing team needs to acquire it, it's reused on every run)\n\n# Steps\nTo quickly summarize the steps needed to set up the checks:\n1. Acquire a license for Understand from Scitools.\n2. Add the license to your repository secrets.\n3. Specify commands, paths, and languages to run Understand and Simian checks.\n4. Create a GitHub Actions workflow file.\n5. Few repo tweaks.\n\n## Step 1 - Acquire a license for Understand from Scitools\n\nYou can acquire a free license for Understand from Scitools as a student. Follow the instructions on their website: https://scitools.com/student\n\n## Step 2 - Add the license to your repository secrets\n\nOnce you have the license string, navigate to your GitHub repository, go to \"Settings\" > \"Secrets and variables\" > \"Actions\", and under \"Repository secrets\" add a new secret named `UNDERSTAND_LICENSE` with the license string as its value.\n\n## Step 3 - Add python script, specify commands, paths, and languages to run Understand and Simian checks\n\nBefore creating the workflow file, we need to prepare a few things in the repository.\n\n1. Create a folder `.github/quality_tools/` in your repository to store the necessary files.\n2. Add to that folder a Python script `parse_metrics.py` to parse the metrics CSV file generated by Understand into a markdown format suitable for GitHub comments. You can download the ready-made script here: [download parse_metrics.py](downloads/parse_metrics.py).\n\n> Place it in `.github/quality_tools/parse_metrics.py` in your repository.\n\nNext, we have to specify the commands for Understand to run the analysis. We create a template file named `frontend_commands.txt` in the same `.github/quality_tools/` folder.\n\n```aiignore\ncreate -languages Web frontend_metrics.und #TODO: Adjust the name and language\n-db frontend_metrics.und #TODO: Adjust the name if you have changed it above\nadd frontend/src\nsettings -MetricShowAggregatedFileMetrics on\nsettings -MetricShowAggregatedClassMetrics on\nsettings -MetricShowCouplingAndCohesionMetrics on\nsettings -MetricShowInheritanceMetrics on\nsettings -MetricShowStatementCountMetrics on\nsettings -MetricCyclomatic all\nsettings -MetricShowDefaultSummaryMetrics on\nanalyze\nmetrics\nreport\n```\nHere to adjust to your project, you might want to change:\n1. Language (e.g. `-languages C++` for C++ projects), we use 'Web' for our frontend project. List of supported languages can be found [here](https://support.scitools.com/support/solutions/articles/70000582794-supported-languages)\n2. Not needed, but you can also adjust the name of the database, but it should match in both places, and will have to be adjusted in templats later.\n\n## Step 4 - Create a GitHub Actions workflow file\n\nEither manually in a folder `.github/workflows/` or using the GitHub interface, create a new workflow file named `code_quality.yml`.\nThis will serve as the configuration for the CI pipeline, which we use to run checks but can be extended to do more in the future, this tutorial serves as a basis for creation just for code quality checks.\n\n\nHere I provide a sample workflow file for our frontend that you can use as a starting point. You can customize it according to your project's needs.\nLook at the comments (specifically those starting with `TODO`) for places where you might want to/should adjust the configuration.\n\n[download frontend_code_quality_job.yml](downloads/frontend_code_quality_job.yml)\n\n> Make sure it is in `.github/workflows` folder.\n\n## Step 5 - Last repo tweaks\n\n1. Go to your repository settings, under \"Actions\" > \"General\", and ensure that \"Read and write permissions\" is selected for the workflow permissions. This is necessary for the workflow to post comments on pull requests.\n\n# Done!\n\nOnce you have completed these steps, every time a pull request is created or updated in your repository, the GitHub Actions workflow will automatically run the code quality checks using Understand and Simian. The results will be posted as a comment on the pull request, allowing you to easily see any issues that need to be addressed before merging.\n\nAdditionally, you can inspect the results by downloading the artifact uploaded by the job when it succeeds:\n![img_2.png](2025-09-27-automatic-code-quality-checks/img_2.png)\n\nUnzip and open the `index.html` file in your browser to see the full report:\n![img_3.png](2025-09-27-automatic-code-quality-checks/img_3.png)\n\n> Note: This template is to be extended with similarity analyzer, if you want the update you can reach out to me at - m.derylo@student.tue.nl."
  }
]